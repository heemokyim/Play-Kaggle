{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation \nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport regex as re\nimport gc\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79caa077c772704c1f4da1f845d71a507295942d"
      },
      "cell_type": "code",
      "source": "baseline_tree_score = 0.23092278864723115\nbaseline_neuralnetwork_score = 0.5480561937041435",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8812e48b602e0dead03599661f7ea2e8abc1b51d"
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/kaggletutorial/covertype_train.csv')\ntest = pd.read_csv('../input/kaggletutorial/covertype_test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d42871b0d61c07cd1fbf7f758e3123b3211ae2c"
      },
      "cell_type": "code",
      "source": "train_index = train.shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0bc39758165884834523c243bbac805bd3aae9ea"
      },
      "cell_type": "code",
      "source": "lgbm_param =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.06,\n    \"num_leaves\": 16,\n    \"max_depth\": 6,\n    \"colsample_bytree\": 0.7,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def keras_model(input_dims):\n    model = Sequential()\n    \n    model.add(Dense(input_dims, input_dim=input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims//2))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    \n    # output layer (y_pred)\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    \n    # compile this model\n    model.compile(loss='binary_crossentropy', # one may use 'mean_absolute_error' as alternative\n                  optimizer='adam', metrics=['accuracy'])\n    return model\n\ndef keras_history_plot(history):\n    plt.plot(history.history['loss'], 'y', label='train loss')\n    plt.plot(history.history['val_loss'], 'r', label='val loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(loc='upper right')\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6c6068ffbd1e0956f4efe0aeae19ddca3720c7e"
      },
      "cell_type": "code",
      "source": "def baseline_tree_cv(train):\n    train_df = train.copy()\n    y_value = train_df[\"Cover_Type\"]\n    del train_df[\"Cover_Type\"], train_df[\"ID\"]\n    \n    NFOLD = 5\n    folds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\n    total_score = 0\n    best_iteration = 0\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n        train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n        evals_result_dict = {} \n        dtrain = lgbm.Dataset(train_x, label=train_y)\n        dvalid = lgbm.Dataset(valid_x, label=valid_y)\n\n        clf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=3000, valid_sets=[dtrain, dvalid],\n                               early_stopping_rounds=200, evals_result=evals_result_dict, verbose_eval=500)\n\n        predict = clf.predict(valid_x)\n        cv_score = log_loss(valid_y, predict )\n        total_score += cv_score\n        best_iteration = max(best_iteration, clf.best_iteration)\n        print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n        lgbm.plot_metric(evals_result_dict)\n        plt.show()\n        \n    print(\"Best Iteration\", best_iteration)\n    print(\"Total LogLoss\", total_score / NFOLD)\n    print(\"Baseline model Score Diff\", total_score / NFOLD - baseline_tree_score)\n    \n    del train_df\n    \n    return best_iteration\n\ndef baseline_keras_cv(train):\n    train_df = train.copy()\n    y_value = train_df['Cover_Type']\n    del train_df['Cover_Type'], train_df['ID']\n    \n    model = keras_model(train_df.shape[1])\n    callbacks = [\n            EarlyStopping(\n                patience=10,\n                verbose=10)\n        ]\n\n    NFOLD = 5\n    folds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\n    total_score = 0\n    best_epoch = 0\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n        train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n        history = model.fit(train_x.values, train_y.values, nb_epoch=30, batch_size = 64, validation_data=(valid_x.values, valid_y.values), \n                            verbose=1, callbacks=callbacks)\n\n        keras_history_plot(history)\n        predict = model.predict(valid_x.values)\n        null_count = np.sum(pd.isnull(predict) )\n        if null_count > 0:\n            print(\"Null Prediction Error: \", null_count)\n            predict[pd.isnull(predict)] = predict[~pd.isnull(predict)].mean()\n\n        cv_score = log_loss(valid_y, predict )\n        total_score += cv_score\n        best_epoch = max(best_epoch, np.max(history.epoch))\n        print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n        \n    print(\"Best Epoch: \", best_epoch)\n    print(\"Total LogLoss\", total_score/NFOLD)\n    print(\"Baseline model Score Diff\", total_score/NFOLD - baseline_neuralnetwork_score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "649133f3e3cee2b8061acc6fc46f1e075bf7bc11"
      },
      "cell_type": "code",
      "source": "def outlier_binary(frame, col, outlier_range):\n    outlier_feature = col + '_Outlier'\n    frame[outlier_feature] = 0\n    frame.loc[frame[col] > outlier_range, outlier_feature] = 1\n    return frame\n\ndef outlier_divide_ratio(frame, col, outlier_range):\n    outlier_index = frame[col] >= outlier_range\n    outlier_median =  frame.loc[outlier_index, col].median()\n    normal_median = frame.loc[frame[col] < outlier_range, col].median()\n    outlier_ratio = outlier_median / normal_median\n    \n    frame.loc[outlier_index, col] = frame.loc[outlier_index, col]/outlier_ratio\n    return frame\n\ndef frequency_encoding(frame, col):\n    freq_encoding = frame.groupby([col]).size()/frame.shape[0] \n    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequncy'.format(col)})\n    return frame.merge(freq_encoding, on=col, how='left')\n\ndef binning_category_combine_feature(frame, col1, col2, col1_quantile, col2_quantile):\n    print(col1, ' ', col2, 'Bining Combine')\n    col1_quantile = np.arange(0,1.1,col1_quantile)\n    col2_quantile = np.arange(0,1.1,col2_quantile)\n    \n    col1_label = '{}_quantile_label'.format(col1)\n    frame[col1_label] = pd.qcut(frame[col1], q=col1_quantile, labels = ['{}_quantile_{:.1f}'.format(col1, col) for col in col1_quantile][1:])\n    \n    col2_label = '{}_quantile_label'.format(col2)\n    frame[col2_label] = pd.qcut(frame[col2], q=col2_quantile, labels = ['{}_quantile_{:.1f}'.format(col2, col) for col in col2_quantile][1:])\n    \n    combine_label = 'Binnig_{}_{}_Combine'.format(col1, col2)\n    frame[combine_label] = frame[[col1_label, col2_label]].apply(lambda row: row[col1_label] +'_'+ row[col2_label] ,axis=1)\n    for col in [col1_label, col2_label, combine_label]:\n        frame[col] = frame[col].factorize()[0]\n    \n    # del frame[col1_label], frame[col2_label]\n    gc.collect()\n    return frame, [col1_label, col2_label, combine_label]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6d692e75d7ea2d73f34559e835b3d259a0866c49"
      },
      "cell_type": "markdown",
      "source": "# 지금까지 수행했던 전처리과정입니다."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35fbc8a01ac213f049fbfab12235e3b0ea1f5a48"
      },
      "cell_type": "code",
      "source": "def tree_data_preprocessing(train, test):\n    train_index = train.shape[0]\n    all_data = pd.concat([train, test])\n    del all_data['oil_Type']\n\n    all_column_set = set(all_data.columns)\n    category_feature = []\n    for col in all_data.loc[:, all_data.dtypes=='object'].columns:\n        all_data[col] = all_data[col].factorize()[0]\n        category_feature.append(col)\n\n    numerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n\n    all_data['Elevation'] = np.log1p(all_data['Elevation'])\n\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = frequency_encoding(all_data, 'Soil_Type')\n    all_data = frequency_encoding(all_data, 'Wilderness_Area')\n\n    aspect_train = all_data.loc[all_data['Aspect'].notnull()]\n    aspect_test = all_data.loc[all_data['Aspect'].isnull()]\n    del aspect_train[\"Cover_Type\"], aspect_train['ID']\n    del aspect_test[\"Cover_Type\"], aspect_test['ID']\n\n    numerical_feature_woaspect = numerical_feature[:]\n    numerical_feature_woaspect.remove('Aspect')\n\n    sc = StandardScaler()\n    aspect_train[numerical_feature_woaspect] = sc.fit_transform(aspect_train[numerical_feature_woaspect])\n    aspect_test[numerical_feature_woaspect] = sc.transform(aspect_test[numerical_feature_woaspect] )\n\n    y_value = aspect_train['Aspect']\n    del aspect_train['Aspect'], aspect_test['Aspect']\n    \n    knn = KNeighborsRegressor(n_neighbors=7)\n    knn.fit(aspect_train,y_value)\n    predict = knn.predict(aspect_test)\n    \n    sns.distplot(predict)\n    sns.distplot(all_data['Aspect'].dropna())\n    plt.title('KNN Aspect Null Imputation')\n    plt.show()\n    \n    all_data.loc[all_data['Aspect'].isnull(),'Aspect'] = predict\n    \n    all_data['Horizontal_Distance_To_Hydrology'] = all_data['Horizontal_Distance_To_Hydrology']/1000\n    all_data['HF1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Fire_Points'])\n    all_data['HF4'] = all_data['Horizontal_Distance_To_Hydrology'] / all_data['Horizontal_Distance_To_Fire_Points']\n\n    all_data['HR1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['HR4'] = all_data['Horizontal_Distance_To_Hydrology'] / all_data['Horizontal_Distance_To_Roadways']\n\n    all_data['HH1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH3'] = np.log1p(abs(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Vertical_Distance_To_Hydrology']))\n    all_data['HH4'] = all_data['Horizontal_Distance_To_Hydrology'] / all_data['Vertical_Distance_To_Hydrology']\n\n    all_data['FR1'] = all_data['Horizontal_Distance_To_Fire_Points'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR2'] = all_data['Horizontal_Distance_To_Fire_Points'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR3'] = np.log1p(all_data['Horizontal_Distance_To_Fire_Points'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['FR4'] = all_data['Horizontal_Distance_To_Fire_Points'] / all_data['Horizontal_Distance_To_Roadways']\n    \n    all_data['Direct_Distance_Hydrology'] = (all_data['Horizontal_Distance_To_Hydrology']**2+all_data['Vertical_Distance_To_Hydrology']**2)**0.5\n    \n    all_data.loc[np.isinf(all_data['HF4']),'HF4'] = 0\n    all_data.loc[np.isinf(all_data['HR4']),'HR4'] = 0\n    all_data.loc[np.isinf(all_data['HH4']),'HH4'] = 0\n    all_data.loc[np.isinf(all_data['FR4']),'FR4'] = 0\n    all_data[['HF4','HH4']] = all_data[['HF4','HH4']].fillna(0)\n    \n    all_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Aspect', 0.1, 0.1) \n    for col in new_col:\n        all_data = frequency_encoding(all_data, col)\n        \n    train_df = all_data.iloc[:train_index]\n    test_df = all_data.iloc[train_index:]\n    \n    soil_mean_encoding = train_df.groupby(['Soil_Type'])['Cover_Type'].agg({'Soil_Type_Mean':'mean', \n                                                                        'Soil_Type_Std':'std', \n                                                                        'Soil_Type_Size':'size', \n                                                                        'Soil_Type_Sum':'sum'}).reset_index()\n    train_df = train_df.merge(soil_mean_encoding, on='Soil_Type', how='left')\n    test_df = test_df.merge(soil_mean_encoding, on='Soil_Type', how='left')\n    \n    wildness_mean_encoding = train_df.groupby(['Wilderness_Area'])['Cover_Type'].agg({'Wilderness_Area_Mean':'mean', \n                                                                              'Wilderness_Area_Std':'std', \n                                                                              'Wilderness_Area_Size':'size', \n                                                                              'Wilderness_Area_Sum':'sum'}).reset_index()\n    train_df = train_df.merge(wildness_mean_encoding, on='Wilderness_Area', how='left')\n    test_df = test_df.merge(wildness_mean_encoding, on='Wilderness_Area', how='left')\n    \n    del all_data, predict, aspect_train, aspect_test\n    gc.collect()\n    \n    return train_df, test_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "390afb95b4382ad9979325d513273e8c7d329326"
      },
      "cell_type": "code",
      "source": "def nn_data_preprocessing(train, test):\n    train_index = train.shape[0]\n    all_data = pd.concat([train, test])\n    del all_data['oil_Type']\n\n    all_column_set = set(all_data.columns)\n    category_feature = []\n    for col in all_data.loc[:, all_data.dtypes=='object'].columns:\n        all_data[col] = all_data[col].factorize()[0]\n        category_feature.append(col)\n    \n    numerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n    \n    all_data['Elevation'] = np.log1p(all_data['Elevation'])\n\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = frequency_encoding(all_data, 'Soil_Type')\n    all_data = frequency_encoding(all_data, 'Wilderness_Area')\n\n    aspect_train = all_data.loc[all_data['Aspect'].notnull()]\n    aspect_test = all_data.loc[all_data['Aspect'].isnull()]\n    del aspect_train[\"Cover_Type\"], aspect_train['ID']\n    del aspect_test[\"Cover_Type\"], aspect_test['ID']\n\n    numerical_feature_woaspect = numerical_feature[:]\n    numerical_feature_woaspect.remove('Aspect')\n\n    sc = StandardScaler()\n    aspect_train[numerical_feature_woaspect] = sc.fit_transform(aspect_train[numerical_feature_woaspect])\n    aspect_test[numerical_feature_woaspect] = sc.transform(aspect_test[numerical_feature_woaspect] )\n\n    y_value = aspect_train['Aspect']\n    del aspect_train['Aspect'], aspect_test['Aspect']\n\n    knn = KNeighborsRegressor(n_neighbors=7)\n    knn.fit(aspect_train,y_value)\n    predict = knn.predict(aspect_test)\n\n    sns.distplot(predict)\n    sns.distplot(all_data['Aspect'].dropna())\n    plt.title('KNN Aspect Null Imputation')\n    plt.show()\n\n    all_data.loc[all_data['Aspect'].isnull(),'Aspect'] = predict\n    \n    all_data['Horizontal_Distance_To_Hydrology'] = all_data['Horizontal_Distance_To_Hydrology']/1000\n    all_data['HF1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Fire_Points'])\n    all_data['HF4'] = all_data['Horizontal_Distance_To_Hydrology'] / all_data['Horizontal_Distance_To_Fire_Points']\n\n    all_data['HR1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['HR4'] = all_data['Horizontal_Distance_To_Hydrology'] / all_data['Horizontal_Distance_To_Roadways']\n\n    all_data['HH1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH3'] = np.log1p(abs(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Vertical_Distance_To_Hydrology']))\n    all_data['HH4'] = all_data['Horizontal_Distance_To_Hydrology'] / all_data['Vertical_Distance_To_Hydrology']\n\n    all_data['FR1'] = all_data['Horizontal_Distance_To_Fire_Points'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR2'] = all_data['Horizontal_Distance_To_Fire_Points'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR3'] = np.log1p(all_data['Horizontal_Distance_To_Fire_Points'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['FR4'] = all_data['Horizontal_Distance_To_Fire_Points'] / all_data['Horizontal_Distance_To_Roadways']\n\n    all_data['Direct_Distance_Hydrology'] = (all_data['Horizontal_Distance_To_Hydrology']**2+all_data['Vertical_Distance_To_Hydrology']**2)**0.5\n    \n    all_data.loc[np.isinf(all_data['HF4']),'HF4'] = 0\n    all_data.loc[np.isinf(all_data['HR4']),'HR4'] = 0\n    all_data.loc[np.isinf(all_data['HH4']),'HH4'] = 0\n    all_data.loc[np.isinf(all_data['FR4']),'FR4'] = 0\n    \n    all_data[['HF4','HH4']] = all_data[['HF4','HH4']].fillna(0)\n    \n    all_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Aspect', 0.1, 0.1)\n    \n    for col in new_col:\n        all_data = frequency_encoding(all_data, col)\n        \n    all_data.drop(columns=new_col,axis=1,inplace=True)\n    \n    before_one_hot = set(all_data.columns)\n    for col in category_feature:\n        all_data = pd.concat([all_data,pd.get_dummies(all_data[col],prefix=col)],axis=1)\n        \n    one_hot_feature = set(all_data.columns) - before_one_hot\n    \n    train_df = all_data.iloc[:train_index]\n    test_df = all_data.iloc[train_index:]\n    \n    soil_mean_encoding = train_df.groupby(['Soil_Type'])['Cover_Type'].agg({'Soil_Type_Mean':'mean', \n                                                                        'Soil_Type_Std':'std', \n                                                                        'Soil_Type_Size':'size', \n                                                                        'Soil_Type_Sum':'sum'}).reset_index()\n    train_df = train_df.merge(soil_mean_encoding, on='Soil_Type', how='left')\n    test_df = test_df.merge(soil_mean_encoding, on='Soil_Type', how='left')\n    \n    wildness_mean_encoding = train_df.groupby(['Wilderness_Area'])['Cover_Type'].agg({'Wilderness_Area_Mean':'mean', \n                                                                              'Wilderness_Area_Std':'std', \n                                                                              'Wilderness_Area_Size':'size', \n                                                                              'Wilderness_Area_Sum':'sum'}).reset_index()\n    train_df = train_df.merge(wildness_mean_encoding, on='Wilderness_Area', how='left')\n    test_df = test_df.merge(wildness_mean_encoding, on='Wilderness_Area', how='left')\n    \n    train_df.drop(columns=category_feature, axis=1, inplace=True)\n    test_df.drop(columns=category_feature, axis=1, inplace=True)\n    \n    scale_feature = list(set(train_df.columns)-one_hot_feature-set(['Cover_Type','ID']))\n    sc = StandardScaler()\n    train_df[scale_feature] = sc.fit_transform(train_df[scale_feature])\n    test_df[scale_feature] = sc.transform(test_df[scale_feature] )\n    \n    return train_df, test_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "03b40e5c68cdfb606cc016ee9e968b6c6087e6d6"
      },
      "cell_type": "markdown",
      "source": "# 시간관계상 제한했던 LightGBM의 HyperParameter를 조절해보도록 하겠습니다.\n많은 Feature를 추가했기 때문에 좀 더 Iteration을 길게 해줍니다."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94dd63e3e526170d385ae46c75ef36e7e401fa90"
      },
      "cell_type": "code",
      "source": "org_train_df, org_test_df = tree_data_preprocessing(train, test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63d809a975013771bb2669be89a1e7b82d0a621c"
      },
      "cell_type": "code",
      "source": "train_df = org_train_df.copy()\ntest_df = org_test_df.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0bc39758165884834523c243bbac805bd3aae9ea"
      },
      "cell_type": "code",
      "source": "lgbm_param =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 24,\n    \"max_depth\": 6,\n    \"colsample_bytree\": 0.65,\n    \"subsample\": 0.7,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.2,\n    \"nthread\":8\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e48ff9742756fb5226d9bbae1ebce9ab40a7b0d"
      },
      "cell_type": "code",
      "source": "y_value = train_df[\"Cover_Type\"]\ndel train_df[\"Cover_Type\"], train_df[\"ID\"]\ndel test_df[\"Cover_Type\"], test_df[\"ID\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0153327e9a5d07f54d97aa9d2e5cc226c22ac25",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "\"\"\" 시간관계상 CV를 돌리지는 않겠습니다.\nNFOLD = 5\nfolds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\ntotal_score = 0\nbest_iteration = 0\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n    train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n    valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n    evals_result_dict = {} \n    dtrain = lgbm.Dataset(train_x, label=train_y)\n    dvalid = lgbm.Dataset(valid_x, label=valid_y)\n\n    clf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=5000, valid_sets=[dtrain, dvalid],\n                           early_stopping_rounds=200, evals_result=evals_result_dict, verbose_eval=500)\n\n    predict = clf.predict(valid_x)\n    cv_score = log_loss(valid_y, predict )\n    total_score += cv_score\n    best_iteration = max(best_iteration, clf.best_iteration)\n    print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n    lgbm.plot_metric(evals_result_dict)\n    plt.show()\n\nprint(\"Best Iteration\", best_iteration)\nprint(\"Total LogLoss\", total_score / NFOLD)\nprint(\"Baseline model Score Diff\", total_score / NFOLD - baseline_tree_score)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ebcc075df82264c210d15aa64527be4244efaeab"
      },
      "cell_type": "code",
      "source": "dtrain = lgbm.Dataset(train_df, label=y_value)\nclf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=5000)\npredict = clf.predict(test_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "97466076d52fba5ddef786663928128a18e9254e"
      },
      "cell_type": "code",
      "source": "submission = pd.read_csv('../input/kaggletutorial/sample_submission.csv')\nsubmission['Cover_Type'] = predict\nsubmission.to_csv('lgbm_last.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b5c50327516d9f8921a801d98b2cd718dd069136"
      },
      "cell_type": "markdown",
      "source": "# Neural Network Model도 제출해봅니다."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94dd63e3e526170d385ae46c75ef36e7e401fa90"
      },
      "cell_type": "code",
      "source": "org_train_df, org_test_df = nn_data_preprocessing(train, test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f4eb8c79dd02fd412d4418d1d8a3f2315d27901"
      },
      "cell_type": "code",
      "source": "train_df = org_train_df.copy()\ntest_df = org_test_df.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "933bd0fed98166d8f3a806866ec3b4087515534d"
      },
      "cell_type": "code",
      "source": "def keras_model(input_dims):\n    model = Sequential()\n    \n    model.add(Dense(input_dims, input_dim=input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims//2))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims//5))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    # output layer (y_pred)\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    \n    # compile this model\n    model.compile(loss='binary_crossentropy', \n                  optimizer='adam', metrics=['accuracy'])\n    return model\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfa8c8ea47455c2b65e3e6f7c560f5d7a39f4233"
      },
      "cell_type": "code",
      "source": "y_value = train_df['Cover_Type']\ndel train_df['Cover_Type'], train_df['ID']\ndel test_df['Cover_Type'], test_df['ID']\n\nmodel = keras_model(train_df.shape[1])\ncallbacks = [\n        EarlyStopping(\n            patience=10,\n            verbose=10)\n    ]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "355d1849dcf9a1418939282c0d891d0c0e06af28"
      },
      "cell_type": "code",
      "source": "\"\"\" 시간관계상 CV를 돌리지는 않겠습니다.\nNFOLD = 5\nfolds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\ntotal_score = 0\nbest_epoch = 0\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n    train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n    valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n    history = model.fit(train_x.values, train_y.values, nb_epoch=30, batch_size = 64, validation_data=(valid_x.values, valid_y.values), \n                        verbose=1, callbacks=callbacks)\n\n    keras_history_plot(history)\n    predict = model.predict(valid_x.values)\n    null_count = np.sum(pd.isnull(predict) )\n    if null_count > 0:\n        print(\"Null Prediction Error: \", null_count)\n        predict[pd.isnull(predict)] = predict[~pd.isnull(predict)].mean()\n\n    cv_score = log_loss(valid_y, predict )\n    total_score += cv_score\n    best_epoch = max(best_epoch, np.max(history.epoch))\n    print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n\nprint(\"Best Epoch: \", best_epoch)\nprint(\"Total LogLoss\", total_score/NFOLD)\nprint(\"Baseline model Score Diff\", total_score/NFOLD - baseline_neuralnetwork_score)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "d9cdbd66aab79aa0f57a9f87494f4988b8288543"
      },
      "cell_type": "code",
      "source": "history = model.fit(train_df.values, y_value.values, nb_epoch=30, batch_size = 64, verbose=1)\npredict = model.predict(test_df.values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "45a3145c060e738b38b074ff9cf22acb5eb01952"
      },
      "cell_type": "code",
      "source": "submission_nn = pd.read_csv('../input/kaggletutorial/sample_submission.csv')\nsubmission_nn['Cover_Type'] = predict\nsubmission_nn.to_csv('nn_last.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1f7ae728ab219a8a1b6f88f0fc0716f1334c0515"
      },
      "cell_type": "markdown",
      "source": "# Weighted Average"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9994ae624b73ff4251db5556a1b113e14337750d"
      },
      "cell_type": "code",
      "source": "def calculate_correlation(base_df, target_df):\n    source = base_df.copy()\n    source = source.merge(target_df,on='ID')\n    corr_df = source.corr()\n    corr = corr_df.ix['Cover_Type_x']['Cover_Type_y']\n    del corr_df, source\n    return corr",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "b2b05c29e18ed0a64e4c956de3d36045df0c40ee"
      },
      "cell_type": "code",
      "source": "source = submission.copy()\nsource = source.merge(submission_nn,on='ID')\nsource",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c56d1ebc98c4abdedcda14e53073602b2836aed6"
      },
      "cell_type": "code",
      "source": "calculate_correlation(submission, submission_nn)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "06c7a2c77771aa38460f0622e5548c8b01a14cbf"
      },
      "cell_type": "markdown",
      "source": "# Stacking"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b57a82a081579d87baaa7dbb5366ab8c6452a434"
      },
      "cell_type": "code",
      "source": "class SklearnWrapper(object):\n    def __init__(self, clf, params=None, **kwargs):\n        seed = kwargs.get('seed', 0)\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict_proba(x)[:,1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c54bc6ccf15a95ca9729dc81635e53beb9089a6d"
      },
      "cell_type": "code",
      "source": "class LgbmWrapper(object):\n    def __init__(self, params=None, **kwargs):\n        seed = kwargs.get('seed', 0)\n        num_rounds = kwargs.get('num_rounds', 1000)\n        early_stopping = kwargs.get('ealry_stopping', 100)\n        eval_function = kwargs.get('eval_function', None)\n        verbose_eval = kwargs.get('verbose_eval', 100)\n\n        self.param = params\n        self.param['seed'] = seed\n        self.num_rounds = num_rounds\n        self.early_stopping = early_stopping\n        self.eval_function = eval_function\n        self.verbose_eval = verbose_eval\n\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        need_cross_validation = True\n        if x_cross is None:\n            need_cross_validation = False\n\n        if isinstance(y_train, pd.DataFrame) is True:\n            y_train = y_train[y_train.columns[0]]\n            if need_cross_validation is True:\n                y_cross = y_cross[y_cross.columns[0]]\n\n        if need_cross_validation is True:\n            dtrain = lgbm.Dataset(x_train, label=y_train, silent=True)\n            dvalid = lgbm.Dataset(x_cross, label=y_cross, silent=True)\n            self.clf = lgbm.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=dvalid,\n                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n                                  verbose_eval=self.verbose_eval)\n        else:\n            dtrain = lgbm.Dataset(x_train, label=y_train, silent= True)\n            self.clf = lgbm.train(self.param, dtrain, self.num_rounds)\n\n    def predict(self, x):\n        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n\n    def get_params(self):\n        return self.param",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d0886e6eb81062b9320caf595a170970af7fbbf1"
      },
      "cell_type": "code",
      "source": "def get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n    nfolds = kwargs.get('NFOLDS', 5)\n    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n    kfold_random_state = kwargs.get('kfold_random_sate', 0)\n\n    ntrain = x_train.shape[0]\n    ntest = x_test.shape[0]\n\n    kf = StratifiedKFold(n_splits= nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((nfolds, ntest))\n\n    cv_sum = 0\n    try:\n        if clf.clf is not None:\n            print(clf.clf)\n    except:\n        print(clf)\n        print(clf.get_params())\n    \n    for i, (train_index, cross_index) in enumerate(kf.split(x_train, y_train)):\n        x_tr, x_cr = None, None\n        y_tr, y_cr = None, None\n        if isinstance(x_tr, pd.DataFrame):\n            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n        else:\n            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n\n        clf.train(x_tr, y_tr, x_cr, y_cr)\n        oof_train[cross_index] = clf.predict(x_cr)\n        cv_score = eval_func(y_cr, oof_train[cross_index])\n\n        print('Fold %d / ' % (i+1), 'CV-Score: %.6f' % cv_score)\n        cv_sum = cv_sum + cv_score\n\n    score = cv_sum / nfolds\n    print(\"Average CV-Score: \", score)\n    # Using All Dataset, retrain\n    clf.train(x_train, y_train)\n    oof_test = clf.predict(x_test)\n\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6222ddb7efd5b6eab251e3e8b509c6674a50611"
      },
      "cell_type": "code",
      "source": "lgbm_param1 =  {\n    'boosting_type': 'dart',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 31,\n    \"max_depth\": 7,\n    \"colsample_bytree\": 0.8,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8,\n    'drop_rate':0.1, \n    'skip_drop':0.5,\n    'max_drop':50, \n    'top_rate':0.1, \n    'other_rate':0.1\n}\n\nlgbm_param2 =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 10,\n    \"max_depth\": 4,\n    \"colsample_bytree\": 0.5,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8\n}\n\nlgbm_param3 =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 24,\n    \"max_depth\": 6,\n    \"colsample_bytree\": 0.5,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8\n}\n\nrf_params = {\n    'criterion':'gini', 'max_leaf_nodes':24, 'n_estimators':200, 'min_impurity_split':0.0000001,\n    'max_features':0.4, 'max_depth':6, 'min_samples_leaf':20, 'min_samples_split':2,\n    'min_weight_fraction_leaf':0.0, 'bootstrap':True,\n    'random_state':1, 'verbose':False\n    \n}\n\net_parmas = {\n    'criterion':'gini', 'max_leaf_nodes':31, 'n_estimators':200, 'min_impurity_split':0.0000001,\n    'max_features':0.6, 'max_depth':10, 'min_samples_leaf':20, 'min_samples_split':2,\n    'min_weight_fraction_leaf':0.0, 'bootstrap':True,\n    'random_state':1, 'verbose':False \n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11932917401410518d2d397726eadde496dc26ff"
      },
      "cell_type": "code",
      "source": "org_train_df, org_test_df = tree_data_preprocessing(train, test)\ntrain_df = org_train_df.copy()\ntest_df = org_test_df.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f965394588ac5702bfe0c7cf947ad428ddaed568"
      },
      "cell_type": "code",
      "source": "y_value = train_df[\"Cover_Type\"]\ndel train_df[\"Cover_Type\"], train_df[\"ID\"]\ndel test_df[\"Cover_Type\"], test_df[\"ID\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfc9d8366d367a16de7cadab43d9dc1d8d90f253"
      },
      "cell_type": "code",
      "source": "et_model = SklearnWrapper(clf = ExtraTreesClassifier, params=et_parmas)\nrf_model = SklearnWrapper(clf = RandomForestClassifier, params=rf_params)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fcf563621e47fd15240099110000627e2cbfd775"
      },
      "cell_type": "code",
      "source": "et_train, et_test = get_oof(et_model, train_df.values, y_value, test_df.values, log_loss, NFOLDS=3)\nrf_train, rf_test = get_oof(rf_model, train_df.values, y_value, test_df.values, log_loss, NFOLDS=3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "436ed00a76590e02e4664a905dfc912c063419fa"
      },
      "cell_type": "code",
      "source": "x_train_second_layer = np.concatenate((rf_train, et_train), axis=1)\nx_test_second_layer = np.concatenate((rf_test, et_test), axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c4e327bf2fdd139c45f5c6b8a9f377d749e80b2"
      },
      "cell_type": "code",
      "source": "x_train = pd.DataFrame(x_train_second_layer)\nx_test = pd.DataFrame(x_test_second_layer)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec0a92da08093a3241cbcb6c92647cdb9cfc676b"
      },
      "cell_type": "code",
      "source": "lgbm_meta_params = {\n    'boosting':'gbdt', 'num_leaves':28, 'learning_rate':0.03, 'min_sum_hessian_in_leaf':0.1,\n    'max_depth':7, 'feature_fraction':0.6, 'min_data_in_leaf':30, 'poission_max_delta_step':0.7,\n    'bagging_fraction':0.8, 'min_gain_to_split':0, \n    'objective':'binary', 'seed':1,'metric': 'binary_logloss'\n}\n\nNFOLD = 3\nfolds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\ntotal_score = 0\nbest_iteration = 0\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(x_train, y_value)):\n    train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n    valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n    evals_result_dict = {} \n    dtrain = lgbm.Dataset(train_x, label=train_y)\n    dvalid = lgbm.Dataset(valid_x, label=valid_y)\n\n    clf = lgbm.train(lgbm_meta_params, train_set=dtrain, num_boost_round=5000, valid_sets=[dtrain, dvalid],\n                           early_stopping_rounds=200, evals_result=evals_result_dict, verbose_eval=500)\n\n    predict = clf.predict(valid_x)\n    cv_score = log_loss(valid_y, predict )\n    total_score += cv_score\n    best_iteration = max(best_iteration, clf.best_iteration)\n    print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n    lgbm.plot_metric(evals_result_dict)\n    plt.show()\n\nprint(\"Best Iteration\", best_iteration)\nprint(\"Total LogLoss\", total_score / NFOLD)\nprint(\"Baseline model Score Diff\", total_score / NFOLD - baseline_tree_score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31a65dfa56499201d4d38eb98979e0d2702eaf38"
      },
      "cell_type": "code",
      "source": "dtrain = lgbm.Dataset(x_train, label=y_value)\nclf = lgbm.train(lgbm_meta_params, train_set=dtrain, num_boost_round=5000)\npredict_stacking = clf.predict(x_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b73c9a8f6eee26cca23546ebde8a807cdb74527c"
      },
      "cell_type": "code",
      "source": "submission_stacking = pd.read_csv('../input/kaggletutorial/sample_submission.csv')\nsubmission_stacking['Cover_Type'] = predict_stacking\nsubmission_stacking.to_csv('submission_stacking.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96e75ff4a9a5962ff022302dea133d75e0b901d2"
      },
      "cell_type": "code",
      "source": "submission_et = pd.read_csv('../input/kaggletutorial/sample_submission.csv')\nsubmission_et['Cover_Type'] = et_test\nsubmission_et.to_csv('submission_et.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b02575f7694df0705e321f63f1795b0a0ce48199"
      },
      "cell_type": "code",
      "source": "submission_rf = pd.read_csv('../input/kaggletutorial/sample_submission.csv')\nsubmission_rf['Cover_Type'] = rf_test\nsubmission_rf.to_csv('submission_rf.csv', index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}